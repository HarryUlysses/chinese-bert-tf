#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§çº§æ¨¡å‹è®­ç»ƒå™¨
"""

import tensorflow as tf
import numpy as np
import json
import os
import logging
from typing import Dict, Any, Optional, Tuple, List
from pathlib import Path
from datetime import datetime
import mlflow
import mlflow.tensorflow
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pickle
import yaml

logger = logging.getLogger(__name__)

class ModelTrainer:
    """ç”Ÿäº§çº§æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self.label_encoder = LabelEncoder()
        self.vectorize_layer = None
        self.training_history = None

        # åˆ›å»ºå¿…è¦ç›®å½•
        self.checkpoint_dir = Path(config['checkpoint_dir'])
        self.model_registry_path = Path(config['model_registry_path'])

        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.model_registry_path.mkdir(parents=True, exist_ok=True)

    def build_model(self, vocab_size: int, num_classes: int) -> tf.keras.Model:
        """æ„å»ºæ¨¡å‹æ¶æ„"""
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(self.config['max_sequence_length'],)),
            tf.keras.layers.Embedding(vocab_size, 128, input_length=self.config['max_sequence_length']),
            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
            tf.keras.layers.GlobalAveragePooling1D(),
            tf.keras.layers.Dense(256, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.1),
            tf.keras.layers.Dense(num_classes, activation='softmax')
        ])

        return model

    def prepare_data(self, texts: List[str], labels: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ”„ å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # ç¼–ç æ ‡ç­¾
        labels_encoded = self.label_encoder.fit_transform(labels)

        # æ•°æ®åˆ†å‰²
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels_encoded,
            test_size=0.3,
            random_state=42,
            stratify=labels_encoded
        )

        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(train_texts)}, éªŒè¯æ ·æœ¬: {len(val_texts)}")

        # åˆ›å»ºæ–‡æœ¬å‘é‡åŒ–å±‚
        self.vectorize_layer = tf.keras.layers.TextVectorization(
            max_tokens=self.config.get('vocab_size', 10000),
            output_mode='int',
            output_sequence_length=self.config['max_sequence_length']
        )

        # é€‚é…æ–‡æœ¬æ•°æ®
        self.vectorize_layer.adapt(train_texts)

        # å‘é‡åŒ–æ–‡æœ¬
        X_train = self.vectorize_layer(train_texts)
        X_val = self.vectorize_layer(val_texts)

        logger.info(f"è¯æ±‡è¡¨å¤§å°: {len(self.vectorize_layer.get_vocabulary())}")

        return X_train, X_val, np.array(train_labels), np.array(val_labels)

    def train(self, texts: List[str], labels: List[str]) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        # è®¾ç½®MLflowå®éªŒ
        mlflow.set_experiment("chinese-text-classification")

        with mlflow.start_run() as run:
            # è®°å½•å‚æ•°
            mlflow.log_params(self.config)

            # å‡†å¤‡æ•°æ®
            X_train, X_val, y_train, y_val = self.prepare_data(texts, labels)

            # æ„å»ºæ¨¡å‹
            vocab_size = len(self.vectorize_layer.get_vocabulary())
            num_classes = len(np.unique(y_train))

            self.model = self.build_model(vocab_size, num_classes)

            # ç¼–è¯‘æ¨¡å‹
            self.model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )

            # å›è°ƒå‡½æ•°
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    patience=5,
                    restore_best_weights=True,
                    monitor='val_accuracy'
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    factor=0.1,
                    patience=3,
                    monitor='val_loss'
                ),
                tf.keras.callbacks.ModelCheckpoint(
                    filepath=str(self.checkpoint_dir / 'best_model.h5'),
                    save_best_only=True,
                    monitor='val_accuracy',
                    save_weights_only=False
                ),
                mlflow.tensorflow.MlflowCallback()
            ]

            # è®­ç»ƒæ¨¡å‹
            logger.info("ğŸ“ˆ å¼€å§‹è®­ç»ƒ...")
            self.training_history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=self.config['epochs'],
                batch_size=self.config['batch_size'],
                callbacks=callbacks,
                verbose=1
            )

            # è¯„ä¼°æ¨¡å‹
            val_loss, val_accuracy = self.model.evaluate(X_val, y_val, verbose=0)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆ - éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")

            # è®°å½•æŒ‡æ ‡
            mlflow.log_metrics({
                'val_accuracy': val_accuracy,
                'val_loss': val_loss,
                'vocab_size': vocab_size,
                'num_classes': num_classes
            })

            # ä¿å­˜æ¨¡å‹
            model_info = self.save_model(val_accuracy, num_classes, vocab_size)

            # è®°å½•æ¨¡å‹æ–‡ä»¶ï¼ˆç¡®ä¿æ–‡ä»¶å­˜åœ¨åå†è®°å½•ï¼‰
            model_info_file = Path(model_info['model_path']) / 'model_info.json'
            if model_info_file.exists():
                mlflow.log_artifact(str(model_info_file))

            return model_info

    def save_model(self, val_accuracy: float, num_classes: int, vocab_size: int) -> Dict[str, Any]:
        """ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯"""
        # ç”Ÿæˆæ¨¡å‹ç‰ˆæœ¬
        model_version = f"v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        model_path = self.model_registry_path / model_version

        # åˆ›å»ºæ¨¡å‹ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        model_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜TensorFlowæ¨¡å‹ä¸ºKerasæ ¼å¼
        model_file_path = model_path / 'model.keras'
        self.model.save(str(model_file_path))

        # ä¿å­˜é¢„å¤„ç†ç»„ä»¶
        with open(model_path / 'label_encoder.pkl', 'wb') as f:
            pickle.dump(self.label_encoder, f)

        # ä¿å­˜å‘é‡åŒ–å±‚é…ç½®
        vectorize_config = {
            'max_tokens': getattr(self.vectorize_layer, 'max_tokens', None) or self.config.get('vocab_size', 10000),
            'output_mode': getattr(self.vectorize_layer, 'output_mode', 'int'),
            'output_sequence_length': getattr(self.vectorize_layer, 'output_sequence_length', self.config['max_sequence_length']),
            'vocabulary': self.vectorize_layer.get_vocabulary()
        }

        with open(model_path / 'vectorize_config.json', 'w', encoding='utf-8') as f:
            json.dump(vectorize_config, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è®­ç»ƒå†å²
        if self.training_history:
            history_data = {k: [float(x) for x in v] for k, v in self.training_history.history.items()}
            with open(model_path / 'training_history.json', 'w') as f:
                json.dump(history_data, f, indent=2)

        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_version': model_version,
            'model_type': 'chinese_text_classifier',
            'framework': 'tensorflow',
            'val_accuracy': float(val_accuracy),
            'vocab_size': vocab_size,
            'num_classes': num_classes,
            'classes': self.label_encoder.classes_.tolist(),
            'max_sequence_length': self.config['max_sequence_length'],
            'training_config': self.config,
            'created_at': datetime.now().isoformat(),
            'model_path': str(model_path),
            'status': 'active'
        }

        with open(model_path / 'model_info.json', 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)

        # æ›´æ–°æ¨¡å‹æ³¨å†Œè¡¨
        self._update_model_registry(model_info)

        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        return model_info

    def _update_model_registry(self, model_info: Dict[str, Any]):
        """æ›´æ–°æ¨¡å‹æ³¨å†Œè¡¨"""
        registry_file = self.model_registry_path / 'registry.json'

        if registry_file.exists():
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        else:
            registry = {'models': [], 'latest': None}

        registry['models'].append(model_info)
        registry['latest'] = model_info['model_version']

        # æŒ‰å‡†ç¡®ç‡æ’åº
        registry['models'].sort(key=lambda x: x['val_accuracy'], reverse=True)

        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… æ¨¡å‹æ³¨å†Œè¡¨å·²æ›´æ–°: {model_info['model_version']}")

    def load_best_model(self) -> tf.keras.Model:
        """åŠ è½½æœ€ä½³æ¨¡å‹"""
        registry_file = self.model_registry_path / 'registry.json'

        if not registry_file.exists():
            raise FileNotFoundError("æ¨¡å‹æ³¨å†Œè¡¨ä¸å­˜åœ¨")

        with open(registry_file, 'r', encoding='utf-8') as f:
            registry = json.load(f)

        if not registry['models']:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_info = registry['models'][0]
        model_path = Path(best_model_info['model_path'])
        model_file_path = model_path / 'model.keras'

        self.model = tf.keras.models.load_model(str(model_file_path))

        # åŠ è½½é¢„å¤„ç†ç»„ä»¶
        with open(Path(best_model_info['model_path']) / 'label_encoder.pkl', 'rb') as f:
            self.label_encoder = pickle.load(f)

        logger.info(f"âœ… åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_info['model_version']}")
        return self.model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒé…ç½®
    config = {
        'name': 'chinese-text-classifier',
        'version': '1.0.0',
        'batch_size': 32,
        'max_sequence_length': 128,
        'learning_rate': 0.001,
        'epochs': 10,
        'checkpoint_dir': 'models/checkpoints',
        'model_registry_path': 'models/registry',
        'vocab_size': 10000
    }

    # ç¤ºä¾‹æ•°æ® - 30ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªç±»åˆ«10ä¸ª
    texts = [
        # å¤©æ°”ç±» (10ä¸ª)
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½", "æ˜å¤©ä¼šä¸‹é›¨", "å¤©æ°”æ™´æœ—", "åˆ®é£äº†", "æ°”æ¸©é€‚å®œ",
        "å¯èƒ½è¦ä¸‹é›¨", "é˜³å…‰æ˜åªš", "ä¹Œäº‘å¯†å¸ƒ", "æ¸©å·®å¾ˆå¤§", "ç©ºæ°”è´¨é‡ä¸é”™",

        # ç§‘æŠ€ç±» (10ä¸ª)
        "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£", "æ·±åº¦å­¦ä¹ å¾ˆæ£’", "äººå·¥æ™ºèƒ½å‘å±•", "å¤§æ•°æ®åˆ†æ", "äº‘è®¡ç®—æŠ€æœ¯",
        "åŒºå—é“¾åº”ç”¨", "é‡å­è®¡ç®—", "ç¥ç»ç½‘ç»œ", "ç®—æ³•ä¼˜åŒ–", "æ•°æ®æŒ–æ˜",

        # ç”Ÿæ´»ç±» (10ä¸ª)
        "æˆ‘å–œæ¬¢è¿åŠ¨", "å¥åº·å¾ˆé‡è¦", "æ—©ç¡æ—©èµ·", "å‡è¡¡é¥®é£Ÿ", "å®šæœŸä½“æ£€",
        "é”»ç‚¼èº«ä½“", "ä¿æŒå¥½å¿ƒæƒ…", "å·¥ä½œç”Ÿæ´»å¹³è¡¡", "å……è¶³ç¡çœ ", "è¥å…»æ­é…"
    ]
    labels = [
        # å¤©æ°”ç±»æ ‡ç­¾ (10ä¸ª)
        "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”", "å¤©æ°”",

        # ç§‘æŠ€ç±»æ ‡ç­¾ (10ä¸ª)
        "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€", "ç§‘æŠ€",

        # ç”Ÿæ´»ç±»æ ‡ç­¾ (10ä¸ª)
        "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»", "ç”Ÿæ´»"
    ]

    # è®­ç»ƒæ¨¡å‹
    trainer = ModelTrainer(config)
    model_info = trainer.train(texts, labels)

    print(f"è®­ç»ƒå®Œæˆ: {model_info['model_version']}")
    print(f"éªŒè¯å‡†ç¡®ç‡: {model_info['val_accuracy']:.4f}")